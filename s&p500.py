# -*- coding: utf-8 -*-
"""S&P500.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rP2oIKhnvITePo_NQDHW26NJFPed4m96

# Prédiction des Prix et Actifs du S&P 500 grâce à du Machine Learning simple
"""

pip install fredapi

pip install pmdarima

pip install tensorflow

"""# Imports

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from fredapi import Fred
import requests
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA

"""# Partie 1: Collecte et Préparation des Données

## Collecte des données

### 1. ^GSPC Data Extraction(S&P 500)
"""

#les données historiques sur 5 ans
sp500 = yf.download('^GSPC', start='2019-01-01', end='2024-01-01', interval='1d')
sp500_flat = sp500.copy()  # pour préserver la structure originale
sp500_flat.columns = [col[0] for col in sp500.columns]
sp500 = sp500_flat.reset_index()


sp500

sp500['Adj Close'].plot()

"""### 2. VIX (Volatility Index)"""

#les données du VIX
vix = yf.download('^VIX', start='2019-01-01', end='2024-01-01', interval='1d')

vix = vix[['Close']].rename(columns={'Close': 'VIX'})
vix_flat = vix.copy()
vix_flat.columns = [col[0] for col in vix.columns]
vix = vix_flat.reset_index()
vix

vix['VIX'].plot()

"""### 3. Taux d'intérêt (Fed Funds Rate)"""

# Clé API FRED
fred = Fred(api_key='bf789efaf2de29f1410841d297296d31')

rates = fred.get_series('FEDFUNDS', observation_start='2019-01-02', observation_end='2023-12-29')

# Convertion en DataFrame
rates_df = rates.reset_index()
rates_df.columns = ['Date', 'FedFundsRate']

rates_df = rates_df.set_index('Date').resample('D').ffill()

# Affichage des premières lignes
rates_df

"""Vu qu'il s'agit des données mensuelles, les valeurs manquantes allant du 2023-12-02 jusqu'à 2023-12-29 seront remplies par 5.33"""

Date = sp500['Date'].copy()
rates_df = pd.merge(Date, rates_df, on='Date', how='left').fillna(5.33)
rates_df

rates_df

"""### 4. inflation



"""

api_key = '2669IYGNWWU73XGL'

# URL pour l'indicateur d'inflation
url = f'https://www.alphavantage.co/query?function=INFLATION&apikey={api_key}'

# Récupération des données
response = requests.get(url)
data = response.json()

# Convertion en DataFrame
inflation_data = pd.DataFrame.from_dict(data['data'])
inflation_data['date'] = pd.to_datetime(inflation_data['date'])
inflation_data = inflation_data.rename(columns={'date':'Date','value':'inflation'})
inflation_data = inflation_data.iloc[:5,:]
inflation_data = inflation_data.set_index('Date').resample('D').ffill()

inflation_data

"""La fréquence des valeurs de l'inflation est annuelle donc les valeurs manquantes de 2023 seront égales à 4.11633838374488"""

inflation_data = pd.merge(Date, inflation_data, on='Date', how='left').fillna(4.11633838374488)

"""### 5. PIB"""

gdp_data = fred.get_series('GDPC1', observation_start='2019-01-02',observation_end='2023-12-29')

# Convertion en DataFrame
gdp_df = gdp_data.reset_index()
gdp_df.columns = ['Date', 'RealGDP']

# ajout des valeurs manquantes du dernier trimestre de 2023
new_row = pd.DataFrame({'Date': ['2023-12-29'], 'RealGDP': [22960.600]})
df = pd.concat([gdp_df, new_row], ignore_index=True)

# Convertion de la colonne 'Date' en datetime pour garantir la cohérence
df['Date'] = pd.to_datetime(df['Date'])

# Tri du DataFrame par date
gdp_df = df.sort_values(by='Date').reset_index(drop=True)
gdp_df = gdp_df.set_index('Date').resample('D').ffill()

gdp_df

"""### 6. Jointure des tables"""

merged_df = sp500.merge(vix, on='Date', how='left')
merged_df = merged_df.merge(rates_df, on='Date', how='left')
merged_df = merged_df.merge(gdp_df, on='Date', how='left')
df = merged_df.merge(inflation_data, on='Date', how='left')

df

"""## Prétraitement des données"""

# pour garder une version des données d'origine
data = df.copy()

data

"""#### Missing values"""

import seaborn as sns
sns.heatmap(data.isna())

data.isna().describe()

"""Il n'y a pas de valeurs manquantes dans la table data retenue

#### Analyse de la distribution de la variable dépendante Close
"""

(data['Adj Close']==data['Close']).value_counts()

"""Les deux colonnes Adj Close et Close sont identiques, on garde seule la colonne Close"""

data=data.drop(columns=['Adj Close'])

sns.distplot(data['Close'])

data['Close'].skew()

"""**Non identifiable à une distribution usuelle et présente une asymétrie**

--- Pour rendre la distribution plus symétrique on va appliquer une transformation logarithmique sur les variable de prix pour se retrouver avec des variables de type rendements logarithmiques (log-returns)

![2.PNG](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPwAAABUCAIAAAAphs2zAAAAA3NCSVQICAjb4U/gAAAAX3pUWHRSYXcgcHJvZmlsZSB0eXBlIEFQUDEAAAiZ40pPzUstykxWKCjKT8vMSeVSAANjEy4TSxNLo0QDAwMLAwgwNDAwNgSSRkC2OVQo0QAFmJibpQGhuVmymSmIzwUAT7oVaBst2IwAAAiTSURBVHic7Z3Pi9NaFMdPH2874p3ZjT8Wk9kMowxoR0VFUNCEwa2muBIEMUHcqKN23DmCLbhxM63Qdds/oEpnoWCrODJKigtXCSLiKqH++AP6FscXQpLetJn29j3v+aykuc25t/nec88992TM9Ho9IAiZ+GvSHSAI0ZDoCekg0RPSQaInpINET0gHiZ6QDhI9IR0kekI6SPSEdJDoCekg0RPSQaInpINET0gHiZ6QDhI9IR0kekI6SPSEdJDoCekg0RPSQaInpINET0gHiZ6QDhI9IR0kekI6SPSEdJDoCekg0RPSQaInpINET0gHiZ6QDhI9IR0kekI6SPSEdJDoCekg0RPSQaInpINET0gHiZ6QDhI9IR0kekHUarXl5eVarSbG3Pz8fCaTcRxHjLnBWV5eNk1zwh3rEZH/Pnq0N7csK5vNMsaq1arruhy7CGNM1/VWq5XaYqPRAADDMEKfFwqFWIvZbLZQKIT6NiZs2zYMAwDy+bwAc7GQ6Hu9Xs+2bcYYCs627RHe2bIsxlg2m42VlG/XF6jruqVSCbXYaDTSGVVVFQBiB5LP5/Hm/lWckyj9dOZSUK1WY6elGEj0v1EUZeTux3VdxpiiKBwninar1WrwQ5SmoigpjLZaLQBQVTX2KqotdGfXdXEmhLoxVnBuT8Tfk+h/g099J0FFFFzH+UoK+V3Ej0NSGEU3328geOeoi0VzhUIhhcXUYFctyxJptEeiR9A7jjaat2070Vuj3WgblCZjbFijlmVx3HzvX51F5yEOv1QqDWtxJ+DeQ9d1kUZ7vd7YszeO45imOT09nclkNE3zP8zlcplMJpPJtNvtcfchkTdv3gAAhraxYO4lk8lMT08/e/YMADzPC47L87zQVyqVCgDcvn070e65c+dCn7948QIAcrncsAMpl8sAcP/+/X4Nms0mABw5ciT4of8Izp49O6zFnbCysqIoSr1eF53MGfesUlUV/QpKqtVq4d7O78BQG8d++Ycdjk7XdegfXzYaDV3XbdtGz6SqqmVZfoIFfWf0uxis89fuWL+LYXeKLXXi2hK7sLiui49mIuE1RoCCVxhx4Y0fwCmKgoPUdX3YFXxMosdJmLiNQ9Houh7cm2KXQhEF6i+xA6Fp32q1UASMsRS7i8QtRCigt227Wq3i5DQMQ0zKMrZLgiMccaLHB5zNZke7Wdw5GAcPsub4Uy44BMy0hLaG/YL1WLtBVFUtlUop9DfIFgIXtCCKohiGMcEngj+UyGxpr9f7eyjHmZpOp4P/uHDhwsmTJ0NX2+32+vr6jRs3VlZWxPQnyKdPnwBAUZS5uTl+yw8fPgCAYRjBIbx//x4AlpaWgi2/fv0KAPPz85y7vX37FgB0XR/JMS1uIdbX1zltNjc3AcCyrFBvJ8729rZIc4LKEPABM8auXLkSvfr8+fNms7lnzx4xnQnx6tUr4O5iffDZnD9/3v/E8zzcGh47dizY8suXL4l3e/nyJQAcOnRoyP7G4HnexsaGoiicvW+n0+l2uxCZnxIiSPTo6XO53MzMTPTqxYsXW63WpB4G+r/Tp0/zmzmOgyFEcDna2toCAMZYis6j3ePHj/ObeZ6naRo/x1WpVLrd7tWrVzlt0O/gzopPuVw2TTOxGR/M2vkr/H8KQaLHB3zq1KnYq0tLS9GYJ5ZisZgZhsQbep6HUl5cXAxdyuVywWf/7t07iIjm9evXEJdz3L9/P9+u4zjodxMHXqlUms3m7OwsZwiPHj3qt4r6oP7OnDnDN+d53r179/htEu9QLBYVRdnY2Pj169dObjUmRIjeF1YoPQwA7XZb0zRN0zDBLB501RARn+M49Xo9OEs/fvwIAIcPHw42w4A+ukrs3bsX/p0nseAlfkzV6XTW1tbu3LkDAKZpapq2trYWbYZu/u7du7GrqA/6nQMHDnDalMtlTdO63e7m5iY+l2Fdda1WO3r0KABEN80cBoktR4iIjSwKK3anODs7u3v37nq9fvny5UFutbq6urq6OsK+oauOLvqYkwnOUhTNwYMHg80woMdVolwu79u3D4MfdMzoy2PBjQRfplNTUydOnAAAXdevX7+On0SbPX36NNHN+7HZrl27OM0WFxcVRdne3n7y5Am2xNk7OAsLC1tbWzMzM8VisV6vJ7bH4zlMm4pDQIYIBdQvF4t5QPEFGAj+3MGaE9d1sUvB9J9fkhVKa+LXG41Go9EIlVLipdhsoF/1wK9F81tyzm4GKVd0Xdf3u4nHQKqqpih/iIK/YWIyFDv2Bx5OoR/tN7AUR1QjwbZt/qoaVBKKL9rParXKGGOM5fP5kHxxqkePOX3FD+J3Ej0CTi3+CUNoHeNXlfVzTyqXfj1PFD0eC462nDuRyRecMcY4BVL/XwY5LUqE73dHXpXOWVhaXKLtBxG9X9Yxqv4PyIRFj6eSgitahTFIaTEfvkcYxM0PBda4j+SAdhDR86ugx8eE35HF09DEXPX/lAcPHjDGTNOM1mAOAh4n9UsyttttfPUu8SB5cPC8bMD08Q4pl8vNZjOfz4sxF2TCovdPLjudzqSyluNjZmYGZRRbe5wIJrl//PgBAJ1OJ3TaihUHly5dGk1fAQDg+/fvAIBdNU1zJ1Xfnz9/BoCfP3/GXq3VateuXTMM4+HDh6lNpEfwyhICo7psNqvr+kSq/AQQfDF82O/iVhs3iwISXPg4FEXBV8VT3MGyLFVVgylIVVWDu/n/wovhmV6fd/KFgccff3xBSK1We/z48c2bN4d9NaTdbk9NTQn7fRzH+fbt28LCAv8MoR+e52HIGiTYf03T5ubmbt26NcKobFgmL3qCEAz9sSdCOkj0hHSQ6AnpINET0kGiJ6SDRE9IB4mekA4SPSEdJHpCOkj0hHSQ6AnpINET0kGiJ6SDRE9IB4mekA4SPSEdJHpCOkj0hHSQ6Anp+AfsFAguK5Qx7wAAAABJRU5ErkJggg==)
"""

data['Log_Close'] = np.log(data['Close'] / data['Close'].shift(1))

sns.distplot(data['Log_Close'])

data['Log_Close'].skew()

"""Généralisation de la transformation aux variables de type prix : High/ Low/ Open/ RealGDP"""

data['Log_High'] = np.log(data['High'] / data['High'].shift(1))
data['Log_Low'] = np.log(data['Low'] / data['Low'].shift(1))
data['Log_Open'] = np.log(data['Open'] / data['Open'].shift(1))
data['Log_RealGDP'] = np.log(data['RealGDP'] / data['RealGDP'].shift(1))

plt.figure(figsize=(16, 14))
plt.subplot(4, 2, 1)
sns.distplot(data['High'])
plt.subplot(4, 2, 2)
sns.distplot(data['Log_High'])
plt.subplot(4, 2, 3)
sns.distplot(data['Low'])
plt.subplot(4, 2, 4)
sns.distplot(data['Log_Low'])
plt.subplot(4, 2, 5)
sns.distplot(data['Open'])
plt.subplot(4, 2, 6)
sns.distplot(data['Log_Open'])
plt.subplot(4, 2, 7)
sns.distplot(data['Close'])
plt.subplot(4, 2, 8)
sns.distplot(data['Log_Close'])
plt.show()

"""#### Analyse de stationnarité (test ADF)

##### Test ADF (Augmented Dickey-Fuller)

Le **test ADF (Augmented Dickey-Fuller)** est une méthode statistique utilisée pour évaluer si une série temporelle est **stationnaire** ou non. Une série est dite stationnaire si ses propriétés statistiques (moyenne, variance et autocorrélation) ne changent pas au cours du temps, ce qui est une condition essentielle pour appliquer certains modèles statistiques, comme ARIMA.

###### **Principe du test :**
Le test ADF vérifie l’hypothèse nulle (\(H_0\)) selon laquelle la série possède une racine unitaire, ce qui signifie qu’elle est non stationnaire. Si l’hypothèse nulle est rejetée (\(p < 0.05\)), cela indique que la série est stationnaire.

###### **Interprétation des résultats :**
1. **Statistique ADF** : Indique la force de la stationnarité (plus elle est négative, plus la série est stationnaire).
2. **P-value** :
   - \(p < 0.05\) : La série est stationnaire (on rejette \(H_0\)).
   - \(p \geq 0.05\) : La série est non stationnaire (on ne rejette pas \(H_0\)).
3. **Valeurs critiques** : Aident à comparer la statistique ADF pour décider si la stationnarité est significative aux niveaux de 1%, 5% ou 10%.

###### **Conclusion :**
Le test ADF est un outil clé pour diagnostiquer la stationnarité d'une série temporelle, et il guide souvent les transformations nécessaires (différenciation, log-transformation, etc.) avant de procéder à une modélisation. Il est souvent complété par d'autres outils comme l'ACF et la PACF pour analyser la dépendance temporelle.

**Mise en oeuvre du test**
"""

adf_result = adfuller(data['Log_Close'].dropna())

print(f"Statistique ADF : {adf_result[0]}")
print(f"P-value : {adf_result[1]}")
print("Valeurs critiques :", adf_result[4])

if adf_result[1] < 0.01:
    print("La série est stationnaire (rejette l'hypothèse nulle).")
else:
    print("La série n'est pas stationnaire (ne rejette pas l'hypothèse nulle).")

"""**Résultat immédiat** : Pouvoir modéliser la série temporelle par une série de type ARIMA, en se basant sur la lecture graphique de ACF et PACF"""

# l'ACF
plot_acf(data['Log_Close'].dropna(), lags=20)
plt.title("ACF (Autocorrelation)")
plt.show()

# la PACF
plot_pacf(data['Log_Close'].dropna(), lags=20, method='ywm')
plt.title("PACF (Partial Autocorrelation)")
plt.show()

"""##### Interprétation du PACF
Le PACF montre la corrélation partielle entre la série et ses décalages (lags) en éliminant l’effet des corrélations intermédiaires. Les barres qui dépassent les limites bleues indiquent des corrélations significatives.
Dans le graphique :
- Le lag 1 a une corrélation très forte.
- Quelques lags légèrement significatifs entre 2 et 10.
- Les lags supérieurs à 10 sont insignifiants.

##### Détermination des décalages temporels pertinents
- Inclure le lag 1 est essentiel pour capturer la dépendance immédiate.
- Les lags jusqu'à 10 peuvent être inclus pour modéliser des dépendances significatives.

##### Recommandation pour la modélisation
- Pour un modèle ARIMA, définissez le paramètre \(p\) à 10 (ou moins).
- Pour un modèle AR simple, inclure les 3 premiers lags est suffisant.

## Feature engineering

#### Création des variables Month, Day_of_week, Day_of_month
"""

data['Month'] = data['Date'].dt.month

data['Day_of_Week'] = data['Date'].dt.day_name()

data['Day_of_Month'] = data['Date'].dt.day

"""#### moyenne mobile sur 10 jours glissants à partir de la variable Log_Close"""

data['10_Day_Moving_Avg'] = data['Log_Close'].rolling(window=10).mean()

"""##### **Remarque**: Pour les 9 premières lignes, la valeur sera NaN, car il faut au moins 10 observations pour calculer la moyenne mobile. La colonne 10_Day_Moving_Avg contiendra les moyennes calculées à partir du 10ème jour.

#### RSI (Relative Strength Index)

##### Relative Strength Index (RSI)

Le **RSI (Relative Strength Index)** est un oscillateur technique utilisé en trading pour mesurer la force des gains et des pertes récents sur une période donnée (généralement 14 jours). Il permet d'évaluer si un actif est en **surachat** ou **survente**.

###### **Formule du RSI** :
1. Calcul du RS (Relative Strength) :
   \[
   RS = \frac{\text{Moyenne des gains sur N périodes}}{\text{Moyenne des pertes sur N périodes}}
   \]
2. Calcul du RSI :
   \[
   RSI = 100 - \frac{100}{1 + RS}
   \]

###### **Interprétation** :
- **RSI > 70** : Zone de **surachat** → Potentiel de retournement à la baisse.
- **RSI < 30** : Zone de **survente** → Potentiel de retournement à la hausse.

Le RSI est un outil puissant pour identifier les opportunités de trading et améliorer les stratégies en intégrant les variations récentes des prix.
"""

# Calcul des variations journalières de la colonne 'Close'
Delta = data['Close'].diff()

# Séparation en gains et pertes
Gain = Delta.apply(lambda x: x if x > 0 else 0)
Loss = Delta.apply(lambda x: -x if x < 0 else 0)

# Calcul des moyennes mobiles exponentielles sur 14 jours
N = 14
Avg_Gain = Gain.rolling(window=N).mean()
Avg_Loss = Loss.rolling(window=N).mean()

# Calcul du RSI
RS = Avg_Gain / Avg_Loss
data['RSI'] = 100 - (100 / (1 + RS))

"""#### Moving Average Convergence Divergence (MACD)

Le **MACD (Moving Average Convergence Divergence)** est un indicateur technique utilisé pour identifier les tendances haussières et baissières dans une série temporelle. Il repose sur la différence entre deux moyennes mobiles exponentielles (EMA).

###### **Calcul du MACD** :
1. **Ligne MACD** :
   \[
   MACD = EMA_{12} - EMA_{26}
   \]
   - \(EMA_{12}\) : Moyenne mobile exponentielle sur 12 périodes (rapide).
   - \(EMA_{26}\) : Moyenne mobile exponentielle sur 26 périodes (lente).

2. **Ligne Signal** :
   \[
   Signal = EMA_{9}(MACD)
   \]
   - Moyenne mobile exponentielle de la ligne MACD sur 9 périodes.

3. **Histogramme MACD** :
   \[
   Histogramme = MACD - Signal
   \]
   - Visualise la force et la direction de la tendance.

###### **Interprétation** :
- **Croisement MACD et Signal** :
  - **MACD dépasse Signal** : Signal haussier (acheter).
  - **MACD passe sous Signal** : Signal baissier (vendre).
- **Histogramme** :
  - Histogramme positif : Tendance haussière s’intensifie.
  - Histogramme négatif : Tendance baissière s’intensifie.

###### **Utilisation stratégique** :
- **Confirmation de tendance** : Le MACD peut être utilisé pour confirmer les signaux d'autres indicateurs comme le RSI.
- **Divergences** :
  - Si le prix monte, mais que le MACD baisse, cela peut signaler un affaiblissement de la tendance.

Le MACD est un outil polyvalent qui aide à repérer les retournements de tendance et à renforcer les décisions de trading stratégiques.
```
"""

# Calcul des moyennes mobiles exponentielles
EMA_12 = data['Close'].ewm(span=12, adjust=False).mean()  # EMA rapide (12 périodes)
EMA_26 = data['Close'].ewm(span=26, adjust=False).mean()  # EMA lente (26 périodes)

# Calcul de la ligne MACD
MACD = EMA_12 - EMA_26
data['MACD'] = MACD

# Calcul de la ligne Signal
Signal = MACD.ewm(span=9, adjust=False).mean()

# Calcul de l'histogramme
Histogram = MACD - Signal

# Visualisation des résultats
plt.figure(figsize=(14, 7))
plt.subplot(2, 1, 1)
plt.plot(data['Close'], label='Close Price', color='blue', alpha=0.5)
plt.title('Prix de Fermeture')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(MACD, label='MACD', color='red')
plt.plot(Signal, label='Signal Line', color='green')
plt.bar(data.index, Histogram, label='Histogram', color='gray', alpha=0.5)
plt.legend(loc='upper left')
plt.title('MACD - Moving Average Convergence Divergence')
plt.show()

"""# Partie 2 : Développement du Modèle Prédictif

## Régression Linéaire

On va enlever les variables Date et Close et l'objectif sera de prédir Log_Close
Pour que le modèle prédictif soit réaliste, les prédictions sont effectuées au début de la journé et nous n'avons aucune idée des variables High, Low, Open donc on va les exclure du modèle ainsi que leur logarithmes
"""

X = data.dropna().drop(['Date','Close','Log_Close','High','Low','Open','Log_High','Log_Low','Log_Open'],axis=1)
y = data.dropna()['Log_Close']

"""Encodage de la variable catégorique Day_of_week"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
X['Day_of_Week_Encoded'] = label_encoder.fit_transform(X['Day_of_Week'])
X = X.drop('Day_of_Week',axis=1)

"""Les variables n'ont pas les mêmes échelles(ordre de grandeur) il va falloir Standardiser la matrice X"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)

X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)


# Fitting the OLS model
reg_lin = sm.OLS(y_train, X_train).fit()

print(reg_lin.summary())

"""##### Interprétation des résultats de la régression

Le modèle présente un **R²** de **0.177**, ce qui indique que seulement **17.7% de la variance** de la variable dépendante (`Log_Close`) est expliquée par les variables indépendantes du modèle. Ce résultat suggère que le modèle a une capacité explicative relativement faible et qu'il pourrait manquer des facteurs clés influençant les variations de `Log_Close`.

###### **Variables significatives :**
- Les variables ayant une **p-value < 0.05** sont considérées comme statistiquement significatives, ce qui signifie qu'elles contribuent de manière importante à expliquer les variations de `Log_Close`. Dans ce cas, les variables suivantes sont significatives :
  - `VIX` (p-value = 0.000)
  - `Log_RealGDP` (p-value = 0.000)

###### **Variables non significatives :**
- Certaines variables, comme `FedFundsRate` (p-value = 0.241), `RealGDP` (p-value = 0.787), et `Day_of_Week_Encoded` (p-value = 0.784), n'ont pas d'impact statistiquement significatif sur `Log_Close`. Ces variables n'expliquent pas de manière notable les variations de la cible dans le cadre de ce modèle.

###### **Conclusion :**
Le modèle actuel pourrait éventuellement être amélioré en supprimant les variables non significatives pour réduire le bruit.

"""

# Prédictions sur le jeu de test
y_pred = reg_lin.predict(X_test.reset_index(drop=True))

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE : {mse}")
print(f"R^2 : {r2}")

plt.figure(figsize=(14, 7))
plt.plot(y_test.to_list(), label='Valeurs réelles', color='blue', linewidth=1.5)
plt.plot(y_pred.to_list(), label='Prédictions', color='red', linestyle='--', linewidth=1.5)
plt.legend()
plt.title("Comparaison entre les valeurs réelles et les prédictions de la régression linéaire")
plt.xlabel("Temps")
plt.ylabel("Valeurs")
plt.grid()
plt.show()

variables_non_significatives = ['RealGDP', 'Day_of_Month', 'Day_of_Week_Encoded']
XX_train = X_train.drop(columns=variables_non_significatives)
XX_test = X_test.drop(columns=variables_non_significatives)

XX_train = sm.add_constant(XX_train).reset_index(drop=True)
XX_test = sm.add_constant(XX_test).reset_index(drop=True)

reg_lin2 = sm.OLS(y_train, XX_train).fit()

reg_lin2.summary()

# Prédictions sur le jeu de test
y_pred2 = reg_lin2.predict(XX_test)

mse = mean_squared_error(y_test, y_pred2)
r2 = r2_score(y_test, y_pred2)

print(f"MSE : {mse}")
print(f"R^2 : {r2}")

"""Pas d'amélioration

## Modèle ARIMA

Vu que la série Log_Close est déjà stationnaire alors d=0
On va chercher p et q via la fonction auto_arima
"""

from pmdarima import auto_arima

# Utilisation d'auto_arima pour trouver les meilleurs paramètres
model = auto_arima(
    data['Log_Close'].dropna(),
    start_p=1, max_p=10,                    # Recherche pour p entre 1 et 10 (lags)
    start_q=0, max_q=10,                     # Recherche pour q entre 1 et 5
    d=0,                                    # Série supposée stationnaire (pas de différenciation nécessaire)
    seasonal=False,
    stepwise=True,                          # Recherche pas à pas pour optimiser la vitesse
    trace=True                              # Afficher les résultats des tests
)

print("Meilleurs paramètres (p, d, q):", model.order)

"""**Best model:**  ARIMA(2,0,0)

**Meilleurs paramètres** (p, d, q): (2, 0, 0)
"""

# le modèle ARIMA
model = ARIMA(data['Log_Close'].dropna(), order=(2, 0, 0))
results = model.fit()

plt.figure(figsize=(42, 12))
plt.plot(data['Log_Close'].dropna(), label='Log-Returns (Données originales)', color='blue', linewidth=1.5)
plt.plot(results.fittedvalues, label='ARIMA Modèle', color='red', linestyle='--', linewidth=1.5)
plt.legend()
plt.title("Comparaison entre le modèle ARIMA et les données originales")
plt.xlabel("Temps")
plt.ylabel("Log-Returns")
plt.grid()
plt.show()

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
r2 = r2_score(y_test, y_pred_rf)
print(f"R^2 : {r2}")

"""### validation croisée (n_estimators)"""

from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid={'n_estimators': np.arange(10, 100, 5)},  # Nombre d'estimateurs
    scoring='r2',  # R² comme métrique
    cv=5           # Validation croisée à 5 folds
)

grid.fit(X_train,y_train)
print(grid.best_params_)
print(grid.best_score_)

grid.best_estimator_.feature_importances_
plt.barh(X_train.columns, grid.best_estimator_.feature_importances_)
plt.xlabel("Importance")
plt.ylabel("Variable")
plt.show()

"""Les variables MACD, RSI, 10 days moving avr, VIX et volume sont les plus à expliquer la target

## Gradient Boosting
"""

from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

y_pred_gb = gb_model.predict(X_test)
r2 = r2_score(y_test, y_pred_gb)
print(f"R^2 : {r2}")

"""### validation croisée (n_estimators)"""

grid = GridSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_grid={'n_estimators': np.arange(10, 100, 5)},  # Nombre d'estimateurs
    scoring='r2',  # R² comme métrique
    cv=5           # Validation croisée à 5 folds
)

grid.fit(X_train,y_train)
print(grid.best_params_)
print(grid.best_score_)

grid.best_estimator_.feature_importances_
plt.barh(X_train.columns, grid.best_estimator_.feature_importances_)
plt.xlabel("Importance")
plt.ylabel("Variable")
plt.show()

"""## LSTM (Long Short-Term Memory)"""

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

données = data['Log_Close'].dropna().values.reshape(-1, 1)

# Normalisation des données
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(données)

# Création des séquences pour LSTM
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

seq_length = 10  # Longueur des séquences
X, y = create_sequences(data_scaled, seq_length)

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# le modèle séquentiel LSTM
model = Sequential([
    LSTM(50, activation='relu', input_shape=(seq_length, 1)),
    Dense(1)
])


model.compile(optimizer='adam', loss='mse')

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16, verbose=1)

# Prédictions sur l'ensemble de test
predictions = model.predict(X_test)

y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))
predictions_rescaled = scaler.inverse_transform(predictions)

# Visualisation des résultats

plt.figure(figsize=(10, 6))
plt.plot(y_test_rescaled, label='Valeurs Réelles', color='blue')
plt.plot(predictions_rescaled, label='Prédictions LSTM', color='red', linestyle='--')
plt.legend()
plt.title('Prédictions LSTM vs Valeurs Réelles')
plt.xlabel('Temps')
plt.ylabel('Valeurs')
plt.grid()
plt.show()

r2 = r2_score(y_test_rescaled, predictions_rescaled)
print(f"R^2 : {r2}")

"""# Pipeline

### Étape 1 : Chargement et préparation des données
"""

def load_and_prepare_data():
    #GSPC
    sp500 = yf.download('^GSPC', start='2019-01-01', end='2024-01-01', interval='1d')
    sp500.columns = [col[0] for col in sp500.columns]
    sp500 = sp500.reset_index()

    #VIX
    vix = yf.download('^VIX', start='2019-01-01', end='2024-01-01', interval='1d')
    vix = vix[['Close']].rename(columns={'Close': 'VIX'})
    vix.columns = [col[0] for col in vix.columns]
    vix = vix.reset_index()

    #Fed Funds Rate
    fred = Fred(api_key='bf789efaf2de29f1410841d297296d31')
    rates = fred.get_series('FEDFUNDS', observation_start='2019-01-02', observation_end='2023-12-29')
    rates_df = rates.reset_index()
    rates_df.columns = ['Date', 'FedFundsRate']
    rates_df = rates_df.set_index('Date').resample('D').ffill()
    Date = sp500['Date'].copy()
    rates_df = pd.merge(Date, rates_df, on='Date', how='left').fillna(5.33)

    #inflation
    api_key = '2669IYGNWWU73XGL'
    url = f'https://www.alphavantage.co/query?function=INFLATION&apikey={api_key}'
    response = requests.get(url)
    data = response.json()
    inflation_data = pd.DataFrame.from_dict(data['data'])
    inflation_data['date'] = pd.to_datetime(inflation_data['date'])
    inflation_data = inflation_data.rename(columns={'date':'Date','value':'inflation'})
    inflation_data = inflation_data.iloc[:5,:]
    inflation_data = inflation_data.set_index('Date').resample('D').ffill()
    inflation_data = pd.merge(Date, inflation_data, on='Date', how='left').fillna(4.11633838374488)

    #PIB

    gdp_data = fred.get_series('GDPC1', observation_start='2019-01-02',observation_end='2023-12-29')
    gdp_df = gdp_data.reset_index()
    gdp_df.columns = ['Date', 'RealGDP']
    new_row = pd.DataFrame({'Date': ['2023-12-29'], 'RealGDP': [22960.600]})
    df = pd.concat([gdp_df, new_row], ignore_index=True)
    df['Date'] = pd.to_datetime(df['Date'])
    gdp_df = df.sort_values(by='Date').reset_index(drop=True)
    gdp_df = gdp_df.set_index('Date').resample('D').ffill()

    #Jointure
    merged_df = sp500.merge(vix, on='Date', how='left')
    merged_df = merged_df.merge(rates_df, on='Date', how='left')
    merged_df = merged_df.merge(gdp_df, on='Date', how='left')
    data = merged_df.merge(inflation_data, on='Date', how='left')

    return data

"""### Étape 2 : Préparation des features et de la cible"""

def prepare_features_and_target(data):
      #  Rendements log
      data['Log_Close'] = np.log(data['Close'] / data['Close'].shift(1))
      data['Log_High'] = np.log(data['High'] / data['High'].shift(1))
      data['Log_Low'] = np.log(data['Low'] / data['Low'].shift(1))
      data['Log_Open'] = np.log(data['Open'] / data['Open'].shift(1))
      data['Log_RealGDP'] = np.log(data['RealGDP'] / data['RealGDP'].shift(1))

      # Variables temporelles
      data['Month'] = data['Date'].dt.month
      data['Day_of_Week'] = data['Date'].dt.day_name()
      data['Day_of_Month'] = data['Date'].dt.day

      # Moyenne mobile sur 10 jours
      data['10_Day_Moving_Avg'] = data['Log_Close'].rolling(window=10).mean()

      # RSI
      Delta = data['Close'].diff()
      Gain = Delta.apply(lambda x: x if x > 0 else 0)
      Loss = Delta.apply(lambda x: -x if x < 0 else 0)
      N = 14
      Avg_Gain = Gain.rolling(window=N).mean()
      Avg_Loss = Loss.rolling(window=N).mean()
      RS = Avg_Gain / Avg_Loss
      data['RSI'] = 100 - (100 / (1 + RS))

      # MACD
      EMA_12 = data['Close'].ewm(span=12, adjust=False).mean()  # EMA rapide (12 périodes)
      EMA_26 = data['Close'].ewm(span=26, adjust=False).mean()  # EMA lente (26 périodes)
      MACD = EMA_12 - EMA_26
      data['MACD'] = MACD

      # Enlever quelques variables
      X = data.dropna().drop(['Date','Adj Close','Close','Log_Close','High','Low','Open','Log_High','Log_Low','Log_Open'],axis=1)
      y = data.dropna()['Log_Close']

      return X, y

"""### Étape 3 : Encodage des variables catégoriques"""

def encoded_data(X):
    X = X[0]
    label_encoder = LabelEncoder()
    X['Day_of_Week_Encoded'] = label_encoder.fit_transform(X['Day_of_Week'])
    X = X.drop('Day_of_Week',axis=1)
    return X

"""### Étape 4 : Standardisation de X"""

def standardize_data(X):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X = pd.DataFrame(X_scaled, columns=X.columns)
    return X

"""### Étape 5 : Entraînement du modèle"""

def train_model(X_train, y_train):
      X_train = sm.add_constant(X_train).reset_index(drop=True)
      y_train = y_train.reset_index(drop=True)
      # Fitting the OLS model
      reg_lin = sm.OLS(y_train, X_train).fit()
      return reg_lin

"""### Étape 6 : Pipeline complète

"""

def pipeline():
    data = prepare_features_and_target(load_and_prepare_data())
    y = data[1]
    X = encoded_data(data)
    X = standardize_data(X)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
    model = train_model(X_train, y_train)

    return model.summary()

pipeline()